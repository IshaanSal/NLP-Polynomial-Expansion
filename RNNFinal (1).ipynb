{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trayxI6lXbz0"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "from typing import Tuple\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import re\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7FluwGFXccn"
      },
      "outputs": [],
      "source": [
        "def load_file(file_path: str) -> Tuple[Tuple[str], Tuple[str]]:\n",
        "    \"\"\" A helper functions that loads the file into a tuple of strings\n",
        "\n",
        "    :param file_path: path to the data file\n",
        "    :return factors: (LHS) inputs to the model\n",
        "            expansions: (RHS) group truth\n",
        "    \"\"\"\n",
        "    data = open(file_path, \"r\").readlines()\n",
        "    factors, expansions = zip(*[line.strip().split(\"=\") for line in data])\n",
        "    return factors, expansions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "No7fyBOfYIg1"
      },
      "outputs": [],
      "source": [
        "text_pairs = []\n",
        "factors, expansions = load_file(\"train.txt\")\n",
        "for i in range(len(factors)):\n",
        "  text_pairs.append(factors[i])\n",
        "  text_pairs.append(expansions[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Le6k_TnLXUlA"
      },
      "outputs": [],
      "source": [
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z282eqj4ZHFU"
      },
      "outputs": [],
      "source": [
        "vocab_size = 4000\n",
        "sequence_length = 29\n",
        "\n",
        "source_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "target_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        ")\n",
        "\n",
        "train_factored_texts = [pair[0] for pair in train_pairs]\n",
        "train_expansion_texts = [pair[1] for pair in train_pairs]\n",
        "source_vectorization.adapt(train_factored_texts)\n",
        "target_vectorization.adapt(train_expansion_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p71WHp8AggZg"
      },
      "outputs": [],
      "source": [
        "batch_size = 12\n",
        "\n",
        "def format_dataset(factored, expanded):\n",
        "factored = source_vectorization(factored)\n",
        "expanded = target_vectorization(expanded)\n",
        "return ({\n",
        "           \"factored\": factored,\n",
        "           \"expanded\": expanded[:, :-1], }, expanded[:, 1:])\n",
        "\n",
        "def make_dataset(pairs):\n",
        "factored_texts = zip(train_factored_texts)\n",
        "expanded_texts = zip(train_expansion_texts)\n",
        "factored_texts = list(factored_texts)\n",
        "expanded_texts = list(expanded_texts)\n",
        "dataset = tf.data.Dataset.from_tensor_slices((factored_texts, expanded_texts))\n",
        "dataset = dataset.batch(batch_size)\n",
        "dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        "return dataset\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XPMnvr3ogh2"
      },
      "outputs": [],
      "source": [
        "embed_dim = 50\n",
        "latent_dim = 400\n",
        "\n",
        "source = keras.Input(shape=(None,), dtype=\"int64\", name=\"Factors\")\n",
        "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source)\n",
        "encoded_source = layers.Bidirectional(\n",
        "    layers.GRU(latent_dim), merge_mode=\"sum\")(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJQSGSukopAG"
      },
      "outputs": [],
      "source": [
        "past_target = keras.Input(shape=(None,), dtype=\"int64\", name=\"Expansions\")\n",
        "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target)\n",
        "decoder_gru = layers.GRU(latent_dim, return_sequences=True)\n",
        "x = decoder_gru(x, initial_state=encoded_source)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "target_next_step = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "seq2seq_rnn = keras.Model([source, past_target], target_next_step)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq_rnn.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "jxM4EaaxJt7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq_rnn.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_kAU0qhjz_U",
        "outputId": "2d4a702d-4c20-4af9-8fb4-ce5bd8bc0e65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " Factors (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " Expansions (InputLayer)        [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding_12 (Embedding)       (None, None, 50)     200000      ['Factors[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_13 (Embedding)       (None, None, 50)     200000      ['Expansions[0][0]']             \n",
            "                                                                                                  \n",
            " bidirectional_6 (Bidirectional  (None, 400)         1084800     ['embedding_12[0][0]']           \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " gru_13 (GRU)                   (None, None, 400)    542400      ['embedding_13[0][0]',           \n",
            "                                                                  'bidirectional_6[0][0]']        \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)            (None, None, 400)    0           ['gru_13[0][0]']                 \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, None, 4000)   1604000     ['dropout_6[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3,631,200\n",
            "Trainable params: 3,631,200\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXeujH_EouTx",
        "outputId": "d5776fae-8450-4f00-fc35-c2b70839663d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/7\n",
            " 1888/21875 [=>............................] - ETA: 1:46:52 - loss: 5.3057e-06 - accuracy: 1.0000"
          ]
        }
      ],
      "source": [
        "seq2seq_rnn.fit(train_ds, epochs=1, validation_data=val_ds)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}